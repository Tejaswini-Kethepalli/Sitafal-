{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQFGaSzeVoQttww6wWXR35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejaswini-Kethepalli/Sitafal-/blob/main/Sitafal_Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUZIumNXU69i",
        "outputId": "be06a6c7-f671-4349-8dda-8b2e484d5a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UEoGuQs_VBMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Crawl and Scrape\n",
        "def crawl_and_scrape(urls):\n",
        "    website_data = {}\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            text = \" \".join([p.get_text() for p in soup.find_all(\"p\")])\n",
        "            website_data[url] = text\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "    return website_data"
      ],
      "metadata": {
        "id": "3jUIMdtQVJT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Chunk and Embed\n",
        "def chunk_and_embed(data, model, chunk_size=300):\n",
        "    chunks = []\n",
        "    for url, content in data.items():\n",
        "        # Split content into chunks of specified size\n",
        "        words = content.split()\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i + chunk_size])\n",
        "            if len(chunk.strip()) > 0:\n",
        "                chunks.append((url, chunk.strip()))\n",
        "    # Generate embeddings for all chunks\n",
        "    texts = [chunk[1] for chunk in chunks]\n",
        "    embeddings = model.encode(texts, convert_to_tensor=False)\n",
        "    return chunks, embeddings"
      ],
      "metadata": {
        "id": "B7gdicsBVQK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Store in FAISS\n",
        "def store_embeddings(embeddings):\n",
        "    d = embeddings.shape[1]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "vW7Im_yhVUtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Query Handling\n",
        "def query_vector_database(query, index, model, chunks, top_k=5):\n",
        "    query_embedding = model.encode([query], convert_to_tensor=False)\n",
        "    distances, indices = index.search(np.array(query_embedding), k=top_k)\n",
        "    results = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "nQebre9ZVZXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate Response\n",
        "def generate_response(results):\n",
        "    if not results:\n",
        "        return \"Sorry, I couldn't find any relevant information.\"\n",
        "    response = \"Based on your query, here are the results:\\n\\n\"\n",
        "    for url, text in results:\n",
        "        response += f\"URL: {url}\\nContent: {text[:200]}...\\n\\n\"  # Truncate long content\n",
        "    return response"
      ],
      "metadata": {
        "id": "5hs4767UVdEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Define URLs\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://und.edu/\",\n",
        "    ]\n",
        "\n",
        "    # Step 2: Initialize embedding model\n",
        "    print(\"Loading embedding model...\")\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Pre-trained model\n",
        "\n",
        "    # Step 3: Crawl and scrape websites\n",
        "    print(\"Crawling and scraping websites...\")\n",
        "    website_data = crawl_and_scrape(urls)\n",
        "\n",
        "    # Step 4: Chunk and embed content\n",
        "    print(\"Chunking and embedding content...\")\n",
        "    chunks, embeddings = chunk_and_embed(website_data, model)\n",
        "\n",
        "    # Step 5: Store embeddings in FAISS\n",
        "    print(\"Storing embeddings in FAISS...\")\n",
        "    index = store_embeddings(embeddings)\n",
        "\n",
        "    # Step 6: Process user query\n",
        "    query = input(\"Enter your query: \")\n",
        "    print(\"Searching content...\")\n",
        "    results = query_vector_database(query, index, model, chunks)\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(results)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeQgPhJ4fyjc",
        "outputId": "26b50fdf-96c4-429c-b143-7b195f659395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Crawling and scraping websites...\n",
            "Error scraping https://www.uchicago.edu/: 403 Client Error: Forbidden for url: https://www.uchicago.edu/\n",
            "Chunking and embedding content...\n",
            "Storing embeddings in FAISS...\n",
            "Enter your query: birds in india\n",
            "Searching content...\n",
            "Generating response...\n",
            "Based on your query, here are the results:\n",
            "\n",
            "URL: https://www.stanford.edu/\n",
            "Content: a vibrant community of creative and accomplished people from around the world A residential campus with diverse housing, exceptional dining, and over 600 student organizations Student Affairs A rich t...\n",
            "\n",
            "URL: https://www.washington.edu/\n",
            "Content: UW astronomy undergrads are using cutting-edge coding skills to help scientists make the most of discoveries from a revolutionary new telescope. Read story Chris Mantegna, â21, is studying how pollu...\n",
            "\n",
            "URL: https://www.stanford.edu/\n",
            "Content: Other ways to search: Map Profiles Stanford Explore Stanford Stanford was founded almost 150 years ago on a bedrock of societal purpose. Our mission is to contribute to the world by educating students...\n",
            "\n",
            "URL: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepar...\n",
            "\n",
            "URL: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepar...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kylkHlSteK62",
        "outputId": "a21bdf3a-0a13-4adf-e9d3-cd91287f55d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def crawl_and_scrape(urls):\n",
        "    \"\"\"\n",
        "    Crawl and scrape content from target websites with appropriate headers.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    content = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "            content.append(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "    return ' '.join(content)\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_size=100):\n",
        "    \"\"\"\n",
        "    Split text into smaller chunks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def store_embeddings(chunks, model):\n",
        "    \"\"\"\n",
        "    Generate embeddings for text chunks and store them in a FAISS index.\n",
        "    \"\"\"\n",
        "    embeddings = model.encode(chunks)\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index, chunks\n",
        "\n",
        "def query_vector_database(query, index, model, chunks, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant chunks using a query.\n",
        "    \"\"\"\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "def generate_response(results):\n",
        "    \"\"\"\n",
        "    Generate a response from retrieved chunks.\n",
        "    \"\"\"\n",
        "    return \"\\n\".join(results)\n",
        "\n",
        "def main():\n",
        "    # Step 1: Define URLs\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://und.edu/\",\n",
        "    ]\n",
        "\n",
        "    # Step 2: Initialize embedding model\n",
        "    print(\"Loading embedding model...\")\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Pre-trained model\n",
        "\n",
        "    # Step 3: Crawl and scrape websites\n",
        "    print(\"Crawling and scraping websites...\")\n",
        "    website_data = crawl_and_scrape(urls)\n",
        "\n",
        "    # Step 4: Chunk and embed content\n",
        "    print(\"Chunking and embedding content...\")\n",
        "    chunks = chunk_text(website_data)\n",
        "    index, stored_chunks = store_embeddings(chunks, model)\n",
        "\n",
        "    # Step 5: Handle user query\n",
        "    query = input(\"Enter your query: \")\n",
        "    print(\"Searching content...\")\n",
        "    results = query_vector_database(query, index, model, stored_chunks)\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(results)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50RJyGXgl7OQ",
        "outputId": "a83b7a22-0509-4524-ba62-3ba3b323b9b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Crawling and scraping websites...\n",
            "Chunking and embedding content...\n",
            "Enter your query: give me about washington\n",
            "Searching content...\n",
            "Generating response...\n",
            "thrilled to welcome him to UW.” Read story Capping a big — and BIG TEN — year, the Huskies are headed for the Tony the Tiger Sun Bowl! Join fellow fans in cheering on our favorite Dawgs against Louisville in El Paso, TX on December 31. Bowl Central David Baker, professor of biochemistry at the UW School of Medicine in Seattle, received the 2024 Nobel Prize in Chemistry. Nobel Week wove stately traditions with imaginative recognitions. Read story © 2024 University of Washington | Seattle, WA Other ways to search: Map Profiles Stanford Explore Stanford Stanford was founded almost 150\n",
            "an international community of scholars working to solve the world's most pressing issues, with initiatives and programs on all seven continents. Chicago is not only in our name, it’s woven into the fabric of this institution. Located in the Hyde Park neighborhood, we benefit from the diversity, arts, and vibrant culture of our South Side community. UW astronomy undergrads are using cutting-edge coding skills to help scientists make the most of discoveries from a revolutionary new telescope. Read story Chris Mantegna, ’21, is studying how pollutants affect shellfish in our food web — and training a new generation of marine\n",
            "years ago on a bedrock of societal purpose. Our mission is to contribute to the world by educating students for lives of leadership and contribution with integrity; advancing fundamental knowledge and cultivating creativity; leading in pioneering research for effective clinical therapies; and accelerating solutions and amplifying their impact. Stories about people, research, and innovation across the Farm Libraries & Archives Science & Engineering Earth & Climate Science & Engineering Health & Medicine Science & Engineering Preparing students to make meaningful contributions to society as engaged citizens and leaders in a complex world Rich learning experiences that provide a broad liberal\n"
          ]
        }
      ]
    }
  ]
}